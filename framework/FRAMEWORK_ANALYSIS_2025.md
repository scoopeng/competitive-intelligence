# Framework Analysis - Current State
**Date**: September 2025
**Status**: Framework Evolution During Competitive Intelligence Re-Assessment

## Key Findings

### 1. Framework Document is OUTDATED
- The `BUSINESS_USER_EMPOWERMENT_FRAMEWORK.md` was created BEFORE the complete competitive intelligence re-assessment
- All 11 competitors were re-scored on September 27, 2025
- The framework evolved during the scoring process

### 2. Framework Inconsistencies

The framework is NOT standardized across competitors. Component names vary significantly:

**Most Common Components (11+ occurrences):**
- Setup (11 competitors)
- Questions (11)
- Portal Prison (11)
- Native Integration (11)
- Investigation (11)
- Interface Simplicity (11)
- Schema Evolution (11)

**Sometimes Present:**
- Speed (14 - but extra occurrences)
- ML/ML Pattern Discovery (10)
- Explanation/Business Explanation (10)
- Data Quality (9)
- Data Prep (9)
- Multi-Source/Multi-Source Analysis (8)

**Unique to Some:**
- Time to First Insight (Domo only - NEW)
- Governed Self-Service (Domo only - NEW)
- Connections (3)
- Writeback (2)

### 3. Current Framework Structure (Most Common Pattern)

Based on analysis of all scoring files:

**Dimension 1: Autonomy (20 points)**
- Setup (8 points)
- Questions (6 points)
- Speed (6 points)

**Dimension 2: Flow (20 points)**
- Native Integration (8 points)
- Portal Prison (6 points)
- Interface Simplicity (6 points)

**Dimension 3: Understanding (20 points)**
- Investigation (8 points)
- ML/ML Pattern Discovery (6 points) - varies
- Explanation/Business Explanation (6 points)

**Dimension 4: Presentation (20 points)**
- Visual Quality/Visuals (6 points) - name varies
- Brand Control/Brand Automation (8 points) - name varies
- Speed & Formats/Distribution (6 points) - name varies

**Dimension 5: Data (20 points)**
- Connections/Multi-Source (4 points) - varies significantly
- Schema Evolution (8 points)
- Data Prep/Data Quality (4 points) - varies
- Writeback (4 points) - sometimes present

### 4. Evolution Patterns

The framework appears to have evolved during scoring:
1. **Early scorers** (Domo, Scoop, Power BI) have more experimental components
2. **Later scorers** (DataChat, Tellius, Zenlytic) have more standardized structure
3. Domo has THREE new components marked with ‚≠ê NEW that don't appear elsewhere

### 5. Scoring Consistency

Despite component name variations, total scores are consistent:
- Scoop: 82/100 (Category A)
- Domo: 62/100 (Category B)
- ThoughtSpot: 57/100 (Category B)
- Qlik: 47/100 (Category C)
- ... (continues down to DataChat: 17/100)

## Recommendations

1. **DO NOT trust the old framework document** - it's pre-re-assessment
2. **Extract the actual framework from scoring files** for accuracy
3. **Handle variations** when building the HTML report
4. **Document the evolution** for transparency

## Next Steps

1. Create a parser that handles framework variations
2. Build HTML report using actual scoring data, not old document
3. Show the framework as it actually exists, with variations noted
4. Deploy as research report with proper disclaimers about methodology evolution